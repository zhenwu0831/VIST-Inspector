{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18424,"status":"ok","timestamp":1701376118582,"user":{"displayName":"Zhen Wu","userId":"18428603457540150845"},"user_tz":300},"id":"TmTYCA-yUBGu","outputId":"d816e8ae-e312-46c0-fd53-314e940ca719"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gH9AxVUfT9La"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import models, transforms\n","from transformers import BertTokenizer, VisualBertForPreTraining\n","import json\n","from PIL import Image\n","from tqdm import tqdm\n","import requests\n","from io import BytesIO\n","import os\n","\n","# Initialize ResNet model without the final fully connected layer\n","resnet_model = models.resnet50(pretrained=True)\n","resnet_model = nn.Sequential(*list(resnet_model.children())[:-1])\n","resnet_model.eval()\n","\n","# Function to request and transform an image\n","def request_image(url, transform):\n","    rsp = requests.get(url, stream=True)\n","    img = Image.open(BytesIO(rsp.content)).convert('RGB')\n","    return transform(img)\n","\n","def download_and_save_image(url, local_path, transform):\n","    response = requests.get(url)\n","    if response.status_code == 200:\n","        with open(local_path, 'wb') as file:\n","            file.write(response.content)\n","        img = Image.open(local_path).convert('RGB')\n","        return transform(img)\n","    else:\n","        raise Exception(\"Failed to download image\")\n","\n","def load_vist_dataset(file_path, image_dir, image_size=(224, 224)):\n","    if not os.path.exists(image_dir):\n","        os.makedirs(image_dir)\n","\n","    with open(file_path, 'r') as file:\n","        vist_data = json.load(file)\n","\n","    transform = transforms.Compose([transforms.Resize(image_size), transforms.ToTensor()])\n","    processed_data = []\n","\n","    for item in tqdm(vist_data):\n","        image_info = item['image_info']\n","        stories = item['stories']\n","        try:\n","            cur_url = image_info['url_o']\n","        except Exception as e:\n","            continue\n","        image_name = os.path.basename(cur_url)\n","        local_image_path = os.path.join(image_dir, image_name)\n","\n","        try:\n","            if not os.path.exists(local_image_path):\n","                # img_tensor = download_and_save_image(cur_url, local_image_path, transform)\n","                continue\n","                # return processed_data\n","                # return None\n","            else:\n","                # return None\n","                img = Image.open(local_image_path).convert('RGB')\n","                img_tensor = transform(img)\n","        except Exception as e:\n","            print(f\"Error processing image {image_name}: {e}\")\n","            continue\n","\n","        for story in stories:\n","            sto = story[0]['text']\n","            processed_data.append({'image': img_tensor, 'story': sto})\n","\n","    return processed_data\n","\n","def load_sorted_dataset_from_directory(directory_path, image_dir, percentage=0.4):\n","    all_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.json')]\n","    all_files.sort()\n","    selected_files = all_files[:int(len(all_files) * percentage)]\n","\n","    dataset = []\n","    for file in selected_files:\n","        cur_load = load_vist_dataset(file, image_dir)\n","        if cur_load == None:\n","            continue\n","        dataset.extend(cur_load)\n","\n","    return dataset\n","\n","# Load the VIST dataset\n","# vist_data_path = 'VIST-Inspector-main/data/VIST/train'\n","# vist_data = load_sorted_dataset_from_directory(vist_data_path)\n","\n","train_img_dir = '/content/gdrive/MyDrive/Project/VIST-Inspector-main/data/VIST/train_img'\n","\n","val_img_dir = '/content/gdrive/MyDrive/Project/VIST-Inspector-main/data/VIST/val_img'\n","\n","train_data_path = '/content/gdrive/MyDrive/Project/VIST-Inspector-main/data/VIST/train'\n","train_data = load_sorted_dataset_from_directory(train_data_path, train_img_dir, 0.6)\n","\n","val_data_path = '/content/gdrive/MyDrive/Project/VIST-Inspector-main/data/VIST/val'\n","val_data = load_sorted_dataset_from_directory(val_data_path, val_img_dir, 1)\n","\n","# Initialize the tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# VIST dataset class\n","class VISTDataset(Dataset):\n","    def __init__(self, data, tokenizer, resnet_model, visual_embedding_dim=768, max_length=512, mask_probability=0.15):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","        self.mask_probability = mask_probability\n","        self.resnet_model = resnet_model\n","        # Projection layer to match VisualBERT's expected visual embedding size\n","        # self.projection = nn.Linear(2048, visual_embedding_dim)\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def mask_tokens(self, inputs):\n","        \"\"\" Randomly mask tokens for MLM with a mask_probability \"\"\"\n","        labels = inputs.clone()\n","        # We sample a few tokens in each sequence for MLM training\n","        probability_matrix = torch.full(labels.shape, self.mask_probability)\n","        masked_indices = torch.bernoulli(probability_matrix).bool()\n","        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n","\n","        # Mask token is the [MASK] token\n","        inputs[masked_indices] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n","\n","        return inputs, labels\n","\n","    def __getitem__(self, idx):\n","        item = self.data[idx]\n","        image = item['image']\n","        story = item['story']\n","\n","        # Extract features using ResNet\n","        with torch.no_grad():\n","            # Inside the __getitem__ method\n","            image_features = self.resnet_model(image.unsqueeze(0))\n","            # print(\"Shape after ResNet:\", image_features.shape)\n","\n","            image_features = image_features.view(image_features.size(0), -1)\n","            # print(\"Shape after flattening:\", image_features.shape)\n","\n","            #image_features = self.projection(image_features)\n","            #print(\"Shape after projection:\", image_features.shape)\n","\n","        # Tokenize text and prepare inputs for VisualBERT\n","        inputs = self.tokenizer.encode_plus(\n","            story,\n","            add_special_tokens=True,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_tensors='pt'\n","        )\n","        inputs_ids, labels = self.mask_tokens(inputs['input_ids'].squeeze())\n","\n","        num_visual_tokens = 1\n","        visual_labels = torch.full((num_visual_tokens,), -100)  # MLM labels for visual tokens\n","        labels = torch.cat([labels, visual_labels], dim=0)\n","\n","        # Ensure that visual_embeds is of correct dimensionality\n","        # image_features = image_features.squeeze(0)  # Remove batch dimension if necessary\n","\n","        # Create an attention mask for the inputs\n","        attention_mask = inputs['attention_mask']\n","\n","        # Create a visual attention mask with the same batch size and the number of visual features\n","        visual_attention_mask = torch.ones((image_features.size(0),), dtype=torch.long).unsqueeze(0)  # Add batch dimension\n","\n","        return {\n","            'input_ids': inputs_ids,\n","            'labels': labels,\n","            'attention_mask': attention_mask,\n","            'visual_embeds': image_features,\n","            'visual_attention_mask': visual_attention_mask  # Add this line\n","        }\n","\n","# Initialize dataset and dataloader\n","# vist_dataset = VISTDataset(vist_data, tokenizer, resnet_model)\n","# dataloader = DataLoader(vist_dataset, batch_size=4, shuffle=True)\n","\n","train_dataset = VISTDataset(train_data, tokenizer, resnet_model)\n","dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n","val_dataset = VISTDataset(val_data, tokenizer, resnet_model)\n","val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n","\n","# Initialize the VisualBERT model\n","model = VisualBertForPreTraining.from_pretrained('uclanlp/visualbert-vqa-coco-pre')\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","# Move model to GPU\n","model.to(device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"miVHRvZt7Loh"},"outputs":[],"source":["# Training loop function\n","def train(model, dataloader, epoch):\n","    model.train()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n","\n","    for batch in dataloader:\n","        input_ids = batch['input_ids']\n","        labels = batch['labels']\n","        visual_embeds = batch['visual_embeds']\n","        attention_mask = batch['attention_mask']  # Add this\n","        visual_attention_mask = batch['visual_attention_mask']  # Add this\n","\n","        # Move inputs to GPU\n","        input_ids = input_ids.to(device)\n","        labels = labels.to(device)\n","        visual_embeds = visual_embeds.to(device)\n","        attention_mask = attention_mask.to(device)  # Add this\n","        visual_attention_mask = visual_attention_mask.to(device)  # Add this\n","\n","        outputs = model(\n","            input_ids=input_ids,\n","            visual_embeds=visual_embeds,\n","            attention_mask=attention_mask,  # Add this\n","            visual_attention_mask=visual_attention_mask,  # Add this\n","            labels=labels\n","        )\n","        loss = outputs.loss\n","\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","    print(f\"Train: Epoch: {epoch}, Loss: {loss.item()}\")\n","\n","def evaluate(model, dataloader, epoch):\n","    model.eval()\n","\n","    with torch.no_grad():\n","        for batch in dataloader:\n","            input_ids = batch['input_ids']\n","            labels = batch['labels']\n","            visual_embeds = batch['visual_embeds']\n","            attention_mask = batch['attention_mask']  # Add this\n","            visual_attention_mask = batch['visual_attention_mask']  # Add this\n","\n","            # Move inputs to GPU\n","            input_ids = input_ids.to(device)\n","            labels = labels.to(device)\n","            visual_embeds = visual_embeds.to(device)\n","            attention_mask = attention_mask.to(device)  # Add this\n","            visual_attention_mask = visual_attention_mask.to(device)  # Add this\n","\n","            outputs = model(\n","                input_ids=input_ids,\n","                visual_embeds=visual_embeds,\n","                attention_mask=attention_mask,  # Add this\n","                visual_attention_mask=visual_attention_mask,  # Add this\n","                labels=labels\n","            )\n","            loss = outputs.loss\n","\n","\n","        print(f\"Dev: Epoch: {epoch}, Loss: {loss.item()}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5308111,"status":"ok","timestamp":1701396178573,"user":{"displayName":"Zhen Wu","userId":"18428603457540150845"},"user_tz":300},"id":"AvZzLT-87UwZ","outputId":"8ba8600c-b420-430d-bafd-679d86077d1e"},"outputs":[{"name":"stderr","output_type":"stream","text":["\r  0%|          | 0/3 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["Train: Epoch: 0, Loss: 0.06654565036296844\n","Dev: Epoch: 0, Loss: 0.09422137588262558\n"]},{"name":"stderr","output_type":"stream","text":["\r 33%|███▎      | 1/3 [29:25<58:50, 1765.06s/it]"]},{"name":"stdout","output_type":"stream","text":["Train: Epoch: 1, Loss: 0.09565964341163635\n","Dev: Epoch: 1, Loss: 0.037996381521224976\n"]},{"name":"stderr","output_type":"stream","text":["\r 67%|██████▋   | 2/3 [58:49<29:24, 1764.93s/it]"]},{"name":"stdout","output_type":"stream","text":["Train: Epoch: 2, Loss: 0.06080478057265282\n","Dev: Epoch: 2, Loss: 0.05732781067490578\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3/3 [1:28:27<00:00, 1769.28s/it]\n"]}],"source":["num_epochs = 3\n","\n","save_path = '/content/gdrive/MyDrive/Project/VIST-Inspector-main/model'\n","\n","def run(model, train_loader, val_loader, epochs):\n","    for epoch in tqdm(range(epochs)):\n","        train(model, train_loader, epoch)\n","        evaluate(model, val_loader, epoch)\n","\n","        torch.save(model.state_dict(), os.path.join(save_path, f'epoch_{epoch}.pt'))\n","\n","run(model, dataloader, val_dataloader, num_epochs)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"V100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
