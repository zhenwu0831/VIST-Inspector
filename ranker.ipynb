{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czMeokEV0Xro",
        "outputId": "6b874f9b-345b-42e9-c4ca-00a61fa175ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrXxSeBA06Yk"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Feci6VctzIW4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torchvision import models, transforms\n",
        "from transformers import BertTokenizer, VisualBertForPreTraining\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import json\n",
        "import os\n",
        "\n",
        "seed = 42\n",
        "\n",
        "def request_image(url, transform):\n",
        "    rsp = requests.get(url, stream=True)\n",
        "    img = Image.open(BytesIO(rsp.content)).convert('RGB')\n",
        "    return transform(img)\n",
        "\n",
        "def load_csv_dataset(file_path, image_size=(224, 224)):\n",
        "    df = pd.read_csv(file_path, nrows=1400)\n",
        "    transform = transforms.Compose([transforms.Resize(image_size), transforms.ToTensor()])\n",
        "    processed_data = []\n",
        "\n",
        "    for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "        story1 = row['sent1']\n",
        "        story2 = row['sent2']\n",
        "        label = row['label']\n",
        "\n",
        "        # Parse the URL field which is in string representation of list\n",
        "        urls = json.loads(row['url'].replace(\"'\", '\"'))\n",
        "\n",
        "        img_tensors = []\n",
        "        for url in urls:\n",
        "            try:\n",
        "                img_tensors.append(request_image(url, transform))\n",
        "            except Exception as e:\n",
        "                # print(f\"Error downloading image: {e}\")\n",
        "                continue\n",
        "\n",
        "        if len(img_tensors) == 0:\n",
        "            continue\n",
        "\n",
        "        img_tensor = torch.stack(img_tensors)\n",
        "\n",
        "        processed_data.append({'image': img_tensor, 'story1': story1, 'story2': story2, 'label': label})\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "\n",
        "# Custom Dataset class\n",
        "class RankingDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, resnet_model, max_length=512, mask_probability=0.15):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.resnet_model = resnet_model\n",
        "        self.max_length = max_length\n",
        "        self.mask_probability = mask_probability\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "\n",
        "        # Process image\n",
        "        image_feature_list = []\n",
        "        # Extract features using ResNet\n",
        "        for image in item['image']:\n",
        "            with torch.no_grad():\n",
        "                # Inside the __getitem__ method\n",
        "                image_features = self.resnet_model(image.unsqueeze(0))\n",
        "                # print(\"Shape after ResNet:\", image_features.shape)\n",
        "\n",
        "                image_features = image_features.view(image_features.size(0), -1)\n",
        "                # print(\"Shape after flattening:\", image_features.shape)\n",
        "\n",
        "                image_feature_list.append(image_features)\n",
        "                #image_features = self.projection(image_features)\n",
        "                #print(\"Shape after projection:\", image_features.shape)\n",
        "\n",
        "        image_features = torch.cat(image_feature_list, dim=0)\n",
        "\n",
        "        # Process text\n",
        "        story1 = item['story1']\n",
        "        story2 = item['story2']\n",
        "\n",
        "        # Tokenize text and prepare inputs for VisualBERT\n",
        "        inputs_1 = self.tokenizer.encode_plus(\n",
        "            story1,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        inputs_2 = self.tokenizer.encode_plus(\n",
        "            story2,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # inputs_ids_1, labels_1 = self.mask_tokens(inputs_1['input_ids'].squeeze())\n",
        "        # inputs_ids_2, labels_2 = self.mask_tokens(inputs_2['input_ids'].squeeze())\n",
        "\n",
        "        inputs_ids_1 = inputs_1['input_ids'].squeeze()\n",
        "        inputs_ids_2 = inputs_2['input_ids'].squeeze()\n",
        "\n",
        "        # num_visual_tokens = len(image_feature_list)\n",
        "        # visual_labels = torch.full((num_visual_tokens,), -100)  # MLM labels for visual tokens\n",
        "\n",
        "        # labels_1 = torch.cat([labels_1, visual_labels], dim=0)\n",
        "        # labels_2 = torch.cat([labels_2, visual_labels], dim=0)\n",
        "\n",
        "        attention_mask_1 = inputs_1['attention_mask']\n",
        "        attention_mask_2 = inputs_2['attention_mask']\n",
        "\n",
        "        visual_attention_mask = torch.ones((image_features.size(0),), dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "        ranker_gap = item['label']\n",
        "\n",
        "        return {\n",
        "            'input_ids_1': inputs_ids_1,\n",
        "            'input_ids_2': inputs_ids_2,\n",
        "            'attention_mask_1': attention_mask_1,\n",
        "            'attention_mask_2': attention_mask_2,\n",
        "            'visual_embeds': image_features,\n",
        "            'visual_attention_mask': visual_attention_mask,\n",
        "            'ranker_gap': ranker_gap\n",
        "        }\n",
        "\n",
        "# Initialize components\n",
        "transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "resnet_model = models.resnet50(pretrained=True)\n",
        "resnet_model = nn.Sequential(*list(resnet_model.children())[:-1])\n",
        "resnet_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLELsajY1Chs",
        "outputId": "84e00ced-fd6c-44e8-f259-54328dffe85d"
      },
      "outputs": [],
      "source": [
        "ranking_data = load_csv_dataset('/content/drive/MyDrive/Colab Notebooks/project/VHED_url.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gn0Ow3i8yctg"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.seed(seed)\n",
        "\n",
        "random.shuffle(ranking_data)\n",
        "\n",
        "train_data_ind = int(0.6 * len(ranking_data))\n",
        "val_data_ind = int(0.8 * len(ranking_data))\n",
        "\n",
        "train_data = ranking_data[:train_data_ind]\n",
        "val_data = ranking_data[train_data_ind:val_data_ind]\n",
        "test_data = ranking_data[val_data_ind:]\n",
        "\n",
        "train_dataset = RankingDataset(train_data, tokenizer, resnet_model)\n",
        "val_dataset = RankingDataset(val_data, tokenizer, resnet_model)\n",
        "test_dataset = RankingDataset(test_data, tokenizer, resnet_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "kt7CFMTQ9fS0"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    # Pad the visual embeddings to have the same sequence length\n",
        "    image_features = pad_sequence([item['visual_embeds'] for item in batch], batch_first=True)\n",
        "\n",
        "    # Get the maximum sequence length for visual_attention_mask\n",
        "    max_seq_length = max(item['visual_attention_mask'].shape[1] for item in batch)\n",
        "\n",
        "    # Pad the visual_attention_mask tensors\n",
        "    visual_attention_mask = [torch.cat([item['visual_attention_mask'], torch.zeros(item['visual_attention_mask'].shape[0], max_seq_length - item['visual_attention_mask'].shape[1])], dim=1) for item in batch]\n",
        "\n",
        "    # Similarly, you can pad other sequences if necessary.\n",
        "    # Stack other values\n",
        "    input_ids_1 = torch.stack([item['input_ids_1'] for item in batch])\n",
        "    input_ids_2 = torch.stack([item['input_ids_2'] for item in batch])\n",
        "    attention_mask_1 = torch.stack([item['attention_mask_1'] for item in batch])\n",
        "    attention_mask_2 = torch.stack([item['attention_mask_2'] for item in batch])\n",
        "    visual_attention_mask = torch.stack(visual_attention_mask)\n",
        "    ranker_gap = torch.tensor([item['ranker_gap'] for item in batch])\n",
        "\n",
        "    return {\n",
        "        'input_ids_1': input_ids_1,\n",
        "        'input_ids_2': input_ids_2,\n",
        "        'attention_mask_1': attention_mask_1,\n",
        "        'attention_mask_2': attention_mask_2,\n",
        "        'visual_embeds': image_features,\n",
        "        'visual_attention_mask': visual_attention_mask,\n",
        "        'ranker_gap': ranker_gap\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "iFKwayviyiLe"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn, num_workers=8)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn, num_workers=8)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn, num_workers=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "kjiPf09o5FP7"
      },
      "outputs": [],
      "source": [
        "# Load your pre-trained VisualBert model\n",
        "model = VisualBertForPreTraining.from_pretrained('uclanlp/visualbert-vqa-coco-pre')\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/project/epoch_8.pt'))\n",
        "model.eval()  # Make sure the model is in evaluation mode\n",
        "\n",
        "# Modify model for ranking task\n",
        "# Add a linear layer to model for regression task\n",
        "model.regression_layer = nn.Linear(model.config.hidden_size, 1)\n",
        "\n",
        "# Define optimizer and loss function for ranking\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
        "criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
        "\n",
        "# Training loop\n",
        "def train_ranker(model, dataloader, optimizer, criterion):\n",
        "    model.train()\n",
        "\n",
        "    total_correct = 0\n",
        "    total_sample = 0\n",
        "\n",
        "    for batch in tqdm(dataloader):\n",
        "        input_ids_1 = batch['input_ids_1']\n",
        "        input_ids_2 = batch['input_ids_2']\n",
        "        attention_mask_1 = batch['attention_mask_1']\n",
        "        attention_mask_2 = batch['attention_mask_2']\n",
        "        visual_embeds = batch['visual_embeds']\n",
        "        visual_attention_mask = batch['visual_attention_mask']\n",
        "        ranking_gap = batch['ranker_gap'].float()\n",
        "\n",
        "        # Forward pass and loss computation for both story inputs\n",
        "        outputs_1 = model(\n",
        "            input_ids=input_ids_1,\n",
        "            visual_embeds=visual_embeds,\n",
        "            attention_mask=attention_mask_1,  # Add this\n",
        "            visual_attention_mask=visual_attention_mask,  # Add this\n",
        "            output_hidden_states=True\n",
        "        )\n",
        "        outputs_2 = model(\n",
        "            input_ids=input_ids_2,\n",
        "            visual_embeds=visual_embeds,\n",
        "            attention_mask=attention_mask_2,  # Add this\n",
        "            visual_attention_mask=visual_attention_mask,  # Add this\n",
        "            output_hidden_states=True\n",
        "        )\n",
        "\n",
        "        # Use the new ranking layer to predict the ranking gap\n",
        "        ranking_prediction_1 = model.regression_layer(outputs_1.hidden_states[-1][:, 0])\n",
        "        ranking_prediction_2 = model.regression_layer(outputs_2.hidden_states[-1][:, 0])\n",
        "        ranking_prediction = ranking_prediction_1 - ranking_prediction_2\n",
        "        ranking_prediction = ranking_prediction.squeeze(-1)\n",
        "\n",
        "        # compute accuracy\n",
        "        for i in range(len(ranking_prediction)):\n",
        "            if ranking_prediction[i] > 0 and ranking_gap[i] > 0:\n",
        "                total_correct += 1\n",
        "            elif ranking_prediction[i] < 0 and ranking_gap[i] < 0:\n",
        "                total_correct += 1\n",
        "\n",
        "        total_sample += ranking_prediction.shape[0]\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(ranking_prediction, ranking_gap)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Train Accuracy: {total_correct / total_sample}\")\n",
        "    print(f\"Train Loss: {loss.item()}\")\n",
        "\n",
        "def evaluate_ranker(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_sample = 0\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader):\n",
        "            input_ids_1 = batch['input_ids_1']\n",
        "            input_ids_2 = batch['input_ids_2']\n",
        "            attention_mask_1 = batch['attention_mask_1']\n",
        "            attention_mask_2 = batch['attention_mask_2']\n",
        "            visual_embeds = batch['visual_embeds']\n",
        "            visual_attention_mask = batch['visual_attention_mask']\n",
        "            ranking_gap = batch['ranker_gap'].float()\n",
        "            # Forward pass\n",
        "            outputs1 = model(\n",
        "                input_ids=input_ids_1,\n",
        "                visual_embeds=visual_embeds,\n",
        "                attention_mask=attention_mask_1,  # Add this\n",
        "                visual_attention_mask=visual_attention_mask,\n",
        "                output_hidden_states=True\n",
        "            )\n",
        "            outputs2 = model(\n",
        "                input_ids=input_ids_2,\n",
        "                visual_embeds=visual_embeds,\n",
        "                attention_mask=attention_mask_2,  # Add this\n",
        "                visual_attention_mask=visual_attention_mask,\n",
        "                output_hidden_states=True\n",
        "            )\n",
        "            #\n",
        "            ranking_prediction_1 = model.regression_layer(outputs1.hidden_states[-1][:, 0])\n",
        "            ranking_prediction_2 = model.regression_layer(outputs2.hidden_states[-1][:, 0])\n",
        "            ranking_prediction = ranking_prediction_1 - ranking_prediction_2\n",
        "            ranking_prediction = ranking_prediction.squeeze(-1)\n",
        "            #\n",
        "            loss = criterion(ranking_prediction, ranking_gap)\n",
        "            total_loss += loss.item()\n",
        "            #\n",
        "            for i in range(len(ranking_prediction)):\n",
        "                if ranking_prediction[i] > 0 and ranking_gap[i] > 0:\n",
        "                    total_correct += 1\n",
        "\n",
        "                elif ranking_prediction[i] < 0 and ranking_gap[i] < 0:\n",
        "                    total_correct += 1\n",
        "            total_sample += ranking_prediction.shape[0]\n",
        "    print(f\"Val Accuracy: {total_correct / total_sample}\")\n",
        "    print(f\"Val Loss: {total_loss / total_sample}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQ3EPkDM0CGB",
        "outputId": "4aa8e296-e0ff-4e95-e636-2d1bb54032cd"
      },
      "outputs": [],
      "source": [
        "num_epochs = 8\n",
        "\n",
        "save_path = '/content/drive/MyDrive/Colab Notebooks/project'\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_ranker(model, train_dataloader, optimizer, criterion)\n",
        "    evaluate_ranker(model, val_dataloader, criterion)\n",
        "    print(f\"Epoch {epoch} complete.\")\n",
        "    # Save the fine-tuned model\n",
        "    torch.save(model.state_dict(), os.path.join(save_path, f'ranker_epoch_{epoch}.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBnGl2qHRfoA"
      },
      "outputs": [],
      "source": [
        "finetuned_model = VisualBertForPreTraining.from_pretrained('uclanlp/visualbert-vqa-coco-pre')\n",
        "\n",
        "finetuned_model.regression_layer = nn.Linear(finetuned_model.config.hidden_size, 1)\n",
        "\n",
        "finetuned_model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/project/ranker_epoch_6.pt'))\n",
        "\n",
        "finetuned_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSARqWHTSU2n",
        "outputId": "9c6fd2fd-2a9f-47ed-f2c0-1da714d89d40"
      },
      "outputs": [],
      "source": [
        "evaluate_ranker(finetuned_model, test_dataloader, criterion)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
