{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czMeokEV0Xro",
        "outputId": "ff2c1793-05a6-4b96-ce6b-1f27b34c48ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrXxSeBA06Yk",
        "outputId": "a4669192-1920-4a0a-c1d6-160e945704b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Feci6VctzIW4",
        "outputId": "d9fbd4b7-3292-4401-ef2e-7ad199af3999"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torchvision import models, transforms\n",
        "from transformers import BertTokenizer, VisualBertForPreTraining\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import json\n",
        "import os\n",
        "\n",
        "\n",
        "def request_image(url, transform):\n",
        "    rsp = requests.get(url, stream=True)\n",
        "    img = Image.open(BytesIO(rsp.content)).convert('RGB')\n",
        "    return transform(img)\n",
        "\n",
        "def load_csv_dataset(file_path, image_size=(224, 224)):\n",
        "    df = pd.read_csv(file_path, nrows=1400)\n",
        "    transform = transforms.Compose([transforms.Resize(image_size), transforms.ToTensor()])\n",
        "    processed_data = []\n",
        "\n",
        "    for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "        story1 = row['sent1']\n",
        "        story2 = row['sent2']\n",
        "        label = row['label']\n",
        "\n",
        "        # Parse the URL field which is in string representation of list\n",
        "        urls = json.loads(row['url'].replace(\"'\", '\"'))\n",
        "\n",
        "        img_tensors = []\n",
        "        for url in urls:\n",
        "            try:\n",
        "                img_tensors.append(request_image(url, transform))\n",
        "            except Exception as e:\n",
        "                # print(f\"Error downloading image: {e}\")\n",
        "                continue\n",
        "\n",
        "        if len(img_tensors) == 0:\n",
        "            continue\n",
        "\n",
        "        img_tensor = torch.stack(img_tensors)\n",
        "\n",
        "        processed_data.append({'image': img_tensor, 'story1': story1, 'story2': story2, 'label': label})\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "\n",
        "# Custom Dataset class\n",
        "class RankingDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, resnet_model, max_length=512, mask_probability=0.15):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.resnet_model = resnet_model\n",
        "        self.max_length = max_length\n",
        "        self.mask_probability = mask_probability\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "\n",
        "        # Process image\n",
        "        image_feature_list = []\n",
        "        # Extract features using ResNet\n",
        "        for image in item['image']:\n",
        "            with torch.no_grad():\n",
        "                # Inside the __getitem__ method\n",
        "                image_features = self.resnet_model(image.unsqueeze(0))\n",
        "                # print(\"Shape after ResNet:\", image_features.shape)\n",
        "\n",
        "                image_features = image_features.view(image_features.size(0), -1)\n",
        "                # print(\"Shape after flattening:\", image_features.shape)\n",
        "\n",
        "                image_feature_list.append(image_features)\n",
        "                #image_features = self.projection(image_features)\n",
        "                #print(\"Shape after projection:\", image_features.shape)\n",
        "\n",
        "        image_features = torch.cat(image_feature_list, dim=0)\n",
        "\n",
        "        # Process text\n",
        "        story1 = item['story1']\n",
        "        story2 = item['story2']\n",
        "\n",
        "        # Tokenize text and prepare inputs for VisualBERT\n",
        "        inputs_1 = self.tokenizer.encode_plus(\n",
        "            story1,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        inputs_2 = self.tokenizer.encode_plus(\n",
        "            story2,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # inputs_ids_1, labels_1 = self.mask_tokens(inputs_1['input_ids'].squeeze())\n",
        "        # inputs_ids_2, labels_2 = self.mask_tokens(inputs_2['input_ids'].squeeze())\n",
        "\n",
        "        inputs_ids_1 = inputs_1['input_ids'].squeeze()\n",
        "        inputs_ids_2 = inputs_2['input_ids'].squeeze()\n",
        "\n",
        "        # num_visual_tokens = len(image_feature_list)\n",
        "        # visual_labels = torch.full((num_visual_tokens,), -100)  # MLM labels for visual tokens\n",
        "\n",
        "        # labels_1 = torch.cat([labels_1, visual_labels], dim=0)\n",
        "        # labels_2 = torch.cat([labels_2, visual_labels], dim=0)\n",
        "\n",
        "        attention_mask_1 = inputs_1['attention_mask']\n",
        "        attention_mask_2 = inputs_2['attention_mask']\n",
        "\n",
        "        visual_attention_mask = torch.ones((image_features.size(0),), dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "        ranker_gap = item['label']\n",
        "\n",
        "        return {\n",
        "            'input_ids_1': inputs_ids_1,\n",
        "            'input_ids_2': inputs_ids_2,\n",
        "            'attention_mask_1': attention_mask_1,\n",
        "            'attention_mask_2': attention_mask_2,\n",
        "            'visual_embeds': image_features,\n",
        "            'visual_attention_mask': visual_attention_mask,\n",
        "            'ranker_gap': ranker_gap\n",
        "        }\n",
        "\n",
        "# Initialize components\n",
        "transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "resnet_model = models.resnet50(pretrained=True)\n",
        "resnet_model = nn.Sequential(*list(resnet_model.children())[:-1])\n",
        "resnet_model.eval()\n",
        "\n",
        "ranking_data = load_csv_dataset('/content/drive/MyDrive/Colab Notebooks/project/VHED_url.csv')\n",
        "\n",
        "ranking_dataset = RankingDataset(ranking_data, tokenizer, resnet_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kt7CFMTQ9fS0"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    # Pad the visual embeddings to have the same sequence length\n",
        "    image_features = pad_sequence([item['visual_embeds'] for item in batch], batch_first=True)\n",
        "\n",
        "    # Get the maximum sequence length for visual_attention_mask\n",
        "    max_seq_length = max(item['visual_attention_mask'].shape[1] for item in batch)\n",
        "\n",
        "    # Pad the visual_attention_mask tensors\n",
        "    visual_attention_mask = [torch.cat([item['visual_attention_mask'], torch.zeros(item['visual_attention_mask'].shape[0], max_seq_length - item['visual_attention_mask'].shape[1])], dim=1) for item in batch]\n",
        "\n",
        "    # Similarly, you can pad other sequences if necessary.\n",
        "    # Stack other values\n",
        "    input_ids_1 = torch.stack([item['input_ids_1'] for item in batch])\n",
        "    input_ids_2 = torch.stack([item['input_ids_2'] for item in batch])\n",
        "    attention_mask_1 = torch.stack([item['attention_mask_1'] for item in batch])\n",
        "    attention_mask_2 = torch.stack([item['attention_mask_2'] for item in batch])\n",
        "    visual_attention_mask = torch.stack(visual_attention_mask)\n",
        "    ranker_gap = torch.tensor([item['ranker_gap'] for item in batch])\n",
        "\n",
        "    return {\n",
        "        'input_ids_1': input_ids_1,\n",
        "        'input_ids_2': input_ids_2,\n",
        "        'attention_mask_1': attention_mask_1,\n",
        "        'attention_mask_2': attention_mask_2,\n",
        "        'visual_embeds': image_features,\n",
        "        'visual_attention_mask': visual_attention_mask,\n",
        "        'ranker_gap': ranker_gap\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 674
        },
        "id": "kjiPf09o5FP7",
        "outputId": "1270e92c-23d0-4a6c-a496-085fb99fed17"
      },
      "outputs": [],
      "source": [
        "dataloader = DataLoader(ranking_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn, num_workers=8)\n",
        "\n",
        "# Load your pre-trained VisualBert model\n",
        "model = VisualBertForPreTraining.from_pretrained('uclanlp/visualbert-vqa-coco-pre')\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/project/epoch_8.pt'))\n",
        "model.eval()  # Make sure the model is in evaluation mode\n",
        "\n",
        "# Modify model for ranking task\n",
        "# Add a linear layer to model for regression task\n",
        "model.regression_layer = nn.Linear(model.config.hidden_size, 1)\n",
        "\n",
        "# Define optimizer and loss function for ranking\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
        "criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
        "\n",
        "# Training loop\n",
        "def train_ranker(model, dataloader, optimizer, criterion):\n",
        "    model.train()\n",
        "\n",
        "    total_correct = 0\n",
        "    total_sample = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        input_ids_1 = batch['input_ids_1']\n",
        "        input_ids_2 = batch['input_ids_2']\n",
        "        attention_mask_1 = batch['attention_mask_1']\n",
        "        attention_mask_2 = batch['attention_mask_2']\n",
        "        visual_embeds = batch['visual_embeds']\n",
        "        visual_attention_mask = batch['visual_attention_mask']\n",
        "        ranking_gap = batch['ranker_gap'].float()\n",
        "\n",
        "        # Forward pass and loss computation for both story inputs\n",
        "        outputs_1 = model(\n",
        "            input_ids=input_ids_1,\n",
        "            visual_embeds=visual_embeds,\n",
        "            attention_mask=attention_mask_1,  # Add this\n",
        "            visual_attention_mask=visual_attention_mask,  # Add this\n",
        "            output_hidden_states=True\n",
        "        )\n",
        "        outputs_2 = model(\n",
        "            input_ids=input_ids_2,\n",
        "            visual_embeds=visual_embeds,\n",
        "            attention_mask=attention_mask_2,  # Add this\n",
        "            visual_attention_mask=visual_attention_mask,  # Add this\n",
        "            output_hidden_states=True\n",
        "        )\n",
        "\n",
        "        # Use the new ranking layer to predict the ranking gap\n",
        "        ranking_prediction_1 = model.regression_layer(outputs_1.hidden_states[-1][:, 0])\n",
        "        ranking_prediction_2 = model.regression_layer(outputs_2.hidden_states[-1][:, 0])\n",
        "        ranking_prediction = ranking_prediction_1 - ranking_prediction_2\n",
        "        ranking_prediction = ranking_prediction.squeeze(-1)\n",
        "\n",
        "        # compute accuracy\n",
        "        for i in range(len(ranking_prediction)):\n",
        "            if ranking_prediction[i] > 0 and ranking_gap[i] > 0:\n",
        "                total_correct += 1\n",
        "            elif ranking_prediction[i] < 0 and ranking_gap[i] < 0:\n",
        "                total_correct += 1\n",
        "\n",
        "        total_sample += ranking_prediction.shape[0]\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(ranking_prediction, ranking_gap)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Accuracy: {total_correct / total_sample}\")\n",
        "    print(f\"Loss: {loss.item()}\")\n",
        "\n",
        "\n",
        "num_epochs = 3\n",
        "\n",
        "save_path = '/content/drive/MyDrive/Colab Notebooks/project'\n",
        "\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    train_ranker(model, dataloader, optimizer, criterion)\n",
        "    print(f\"Epoch {epoch} complete.\")\n",
        "    # Save the fine-tuned model\n",
        "    torch.save(model.state_dict(), os.path.join(save_path, f'ranker_epoch_{epoch}.pt'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
