# -*- coding: utf-8 -*-
"""ranker_pair_regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bQH8uOa_yARNZUD5KAFxhqQCqVdf1_46
"""

'''
*Note: the code about sentence-order-prediction using ALBERT
 and calculating coherence score are adapted from https://github.com/usydnlp/rovist 
'''

!pip install textstat -q

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
from sklearn.model_selection import train_test_split
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
from torchvision import models, transforms
from transformers import BertTokenizer, VisualBertForPreTraining
from PIL import Image
from tqdm import tqdm
import requests
from io import BytesIO
import json
import os
import random
import gc
import h5py
import pickle
import numpy as np
import time


import argparse
import textstat
import spacy
from spacy.lang.en.stop_words import STOP_WORDS
from spacy.lang.en import English
from collections import Counter

DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

!pip install sentencepiece

from transformers import AlbertTokenizer, AlbertModel
import argparse
import pandas as pd
import os
import datetime
import numpy as np
from sklearn.metrics import accuracy_score

import torch
import torchvision
import torch.nn as nn
import torch.nn.functional as F
import torch.autograd as autograd
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import models

use_cuda = True if torch.cuda.is_available() else False
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print('We are using GPU.' if use_cuda else 'We are using CPU.')

model_nm = 'albert-large-v1'
tokenizer = AlbertTokenizer.from_pretrained(model_nm)

# def test_loader(dl, index):
#     """
#     Sanity check the dataloader.
#     """
#     print('Token tensors for this sample:', dl[index]['tokens_tensor'])
#     print('Segment IDs for this sample:', dl[index]['segments_tensor'])
#     print('The class this sample: ', dl[index]['target'])

# class ALBERTDataLoader(Dataset):
#     def __init__(self, dataframe):

#         self.dataframe = dataframe
#         self.prev_sents = dataframe['Sent A'].to_list()
#         self.next_sents = dataframe['Sent B'].to_list()
#         self.targets = dataframe['Class'].to_list()

#     def __len__(self):
#         return len(self.dataframe)

#     def __getitem__(self, idx):

#         sent1 = self.prev_sents[idx]
#         sent2 = self.next_sents[idx]

#         # get sentence lengths
#         sent1_len = len(tokenizer.tokenize(sent1))
#         sent2_len = len(tokenizer.tokenize(sent2))

#         target = self.targets[idx]

#         return sent1, sent2, sent1_len, sent2_len, target

# def collate_fn(data):
#     """
#     Collate data for each mini-batch.
#     Data comes as a list of tuples (prev_sent, next_sent, prev_sent_len,
#     next_sent_len, sentence order label)

#     Returns: token ids, sequence ids, attention masks and labels.
#     """
#     # sort a data list by sequence lenngth
#     data.sort(key=lambda x: x[2]+x[3], reverse=True)
#     sent1, sent2, sent1_len, sent2_len, targets = zip(*data)

#     max_len = sent1_len[0] + sent2_len[0]
#     tokens_tensor, segments_tensor, attentions_tensor = [], [], []
#     sent_order_labels = []

#     for i in range(len(data)):

#         sentA = sent1[i]
#         sentB = sent2[i]

#         # length of the sequence
#         seq_len = sent1_len[i] + sent2_len[i]

#         pad_length = max_len - seq_len
#         text_list = ['[CLS]', sentA, '[SEP]', sentB, '[SEP]']

#         text = ' '.join(text_list)

#         # Tokenize input
#         tokenized_text = tokenizer.tokenize(text)
#         tokenized_text.extend(['[PAD]'] * pad_length)

#         # Convert token to vocabulary indices
#         indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)

#         # 0 for the previous sentence, 1 for the following sentence
#         segments_ids = (sent1_len[i]+2)*[0] + (sent2_len[i]+1)*[1] + (pad_length)*[1]

#         # mask the padding tokens. 0 means to mask the token
#         attention_ids = (sent1_len[i]+2)*[1] + (sent2_len[i]+1)*[1] + (pad_length)*[0]

#         tokens_tensor.append(torch.tensor([indexed_tokens]))
#         segments_tensor.append(torch.tensor([segments_ids]))
#         attentions_tensor.append(torch.tensor([attention_ids]))

#     # Convert inputs to PyTorch tensors and move to device
#     tokens_tensor = torch.stack(tokens_tensor).squeeze(1).to(device)
#     segments_tensor = torch.stack(segments_tensor).squeeze(1).to(device)
#     attentions_tensor = torch.stack(attentions_tensor).squeeze(1).to(device)

#     targets = np.asarray(targets)
#     targets = torch.from_numpy(targets).to(device)

#     return tokens_tensor, segments_tensor, attentions_tensor, targets

class SOPClassifier(nn.Module):
    def __init__(self, hidden_dim, dropout_prob, model_nm):
        """
        Albert Model with head for SOP.
        """
        super(SOPClassifier, self).__init__()

        self.albert_model = AlbertModel.from_pretrained(model_nm)
        self.dropout = nn.Dropout(p=dropout_prob)
        self.linear = nn.Linear(hidden_dim, 2)

    def forward(self, tokens_tensor, attentions_tensor, segments_tensor):

        # pooled_seq shape = (batch_size, 1024)
        pooled_seq = self.albert_model(tokens_tensor,
                                    attention_mask = attentions_tensor,
                                    token_type_ids = segments_tensor)[1]

        # SOP classifier. Outputs are logits
        # pooled_output = self.dropout(pooled_seq)
        output = self.linear(pooled_seq)

        return output

import nltk
import argparse
nltk.download('punkt')

coh_pre_model_pth = "/content/drive/MyDrive/11785_IDL/Project/VIST-Inspector-main/Rovist_pre_train_model/sop_model_epoch4.pth.tar"
coh_checkpoint = torch.load(coh_pre_model_pth)
ckpt_opt = coh_checkpoint['opt']

coh_model = SOPClassifier(ckpt_opt.hidden_dim, ckpt_opt.dropout_prob, model_nm)
coh_model.to(DEVICE)
coh_model.load_state_dict(coh_checkpoint["model"], strict=False)
coh_model.eval()

def coherence_score(text, model, device):
    coherence_scores = []
    sentences = nltk.sent_tokenize(text)
    scores = []

    for i in range(len(sentences)-1):
        sentA = sentences[i]
        sentB = sentences[i+1]

          # repeated sentence --> automatically assign 0 score
        if sentA.strip() == sentB.strip():
          scores.append(0)
          continue

        sentA_len = len(tokenizer.tokenize(sentA))
        sentB_len = len(tokenizer.tokenize(sentB))

        text_list = ['[CLS]', sentA, '[SEP]', sentB, '[SEP]']
        input_text = ' '.join(text_list)

        tokenized_text = tokenizer.tokenize(input_text)

        # token ids
        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)
        token_ids = torch.tensor([indexed_tokens])

        # sequence ids
        segment_ids = (sentA_len+2)*[0] + (sentB_len+1) * [1]
        segment_ids = torch.tensor([segment_ids])

        # attention mask id
        attention_ids = (sentA_len+2)*[1] + (sentB_len+1) * [1]
        attention_ids = torch.tensor([attention_ids])

        # if GPU is available, move data to GPU
        token_ids = token_ids.to(device)
        attention_ids = attention_ids.to(device)
        segment_ids = segment_ids.to(device)

        with torch.no_grad():
            sop_logits = model(token_ids, attention_ids, segment_ids)

        sop_probs = F.softmax(sop_logits, dim=1)

        scores.append(sop_probs[0][1].item())

    return np.mean(scores)

def calc_coh_for_data(data, model, device):
    d_len = len(data)
    for i in range(d_len):
        data[i]['story1_coherence_score'] = coherence_score(item['story1'], model, device)
        data[i]['story2_coherence_score'] = coherence_score(item['story2'], model, device)

    return data

def order_score(text, ngram_len):
    story_rep_scores = []

    sent_tokens = nltk.sent_tokenize(text)

    word_tokens = []

    for sent in sent_tokens:
        # don't include the punctuation at the end.
        word_tokens.append(nltk.word_tokenize(sent)[:-1])

    inter_sentence_scores = []

    for i in range(1, len(word_tokens)):
        next_ngrams = word_tokens[i]

        for j in range(0, i):
            prev_ngrams = word_tokens[j]
            union = len(set(next_ngrams + prev_ngrams))

            intersection = 0
            for token_i in next_ngrams:
                for token_j in prev_ngrams:
                    if token_i == token_j:
                        intersection += 1

            if union != 0:
              inter_sentence_scores.append(intersection/union)

    intra_sentence_scores = []

    for tokens in word_tokens:
        for i in range(0, len(tokens), ngram_len):
            j = i + ngram_len
            prev_slice = tokens[i:i+ngram_len]
            next_slice = tokens[j:j+ngram_len]

            if len(next_slice) == 0: continue
            union = len(set(prev_slice + next_slice))

            intersection = 0
            for token_i in prev_slice:
                for token_j in next_slice:
                    if token_i == token_j:
                        intersection += 1
            if union != 0:
              intra_sentence_scores.append(intersection/union)
    if len(intra_sentence_scores) != 0:
        if len(inter_sentence_scores) != 0:
            repetition_score = [np.mean(inter_sentence_scores), np.mean(intra_sentence_scores)]
        else:
            repetition_score = [0, np.mean(intra_sentence_scores)]
    else: # sentences are way too short to do intra sentence rep
        if len(inter_sentence_scores) != 0:
            repetition_score = [np.mean(inter_sentence_scores), 0]
        else:
            repetition_score = [0, 0]

    return 1-np.mean(repetition_score)

class MultitaskModel(nn.Module):
    def __init__(self, model, class_labels):
        super(MultitaskModel, self).__init__()
        self.model = model
        model.regression_layer = nn.Linear(model.config.hidden_size, 1)
        self.classifier = nn.Linear(model.config.hidden_size, class_labels)

    def forward(self, input_ids, visual_embeds, attention_mask, visual_attention_mask, output_hidden_states):
        outputs = self.model(input_ids=input_ids,
                            visual_embeds=visual_embeds,
                            attention_mask=attention_mask,  # Add this
                            visual_attention_mask=visual_attention_mask,  # Add this
                            output_hidden_states=True)
        outputs = self.model.dropout(outputs.hidden_states[-1][:, 0])

        ranking_prediction_1 = self.model.regression_layer(outputs)
        ranking_prediction = ranking_prediction_1.squeeze(-1)

        classification_logits_1 = self.classifier(outputs)
        classification_logits = classification_logits_1.squeeze(-1)

        return ranking_prediction, classification_logits

seed = 42

nlp = spacy.load("en_core_web_sm")

stop_words = STOP_WORDS

# Feature extraction function
def extract_nlp_features(text):
    # Process the text with spaCy. This includes tokenization, lemmatization, POS tagging, and NER
    doc = nlp(text)

    # Calculate Type-Token Ratio (TTR) for lexical diversity
    tokens = [token.lemma_.lower() for token in doc if token.is_alpha and not token.is_stop]
    unique_words = set(tokens)
    ttr = len(unique_words) / len(tokens) if tokens else 0

    # Sentence complexity (average number of words per sentence)
    sentences = list(doc.sents)
    sentence_complexity = sum(len(sentence) for sentence in sentences) / len(sentences) if sentences else 0

    # Readability score using Flesch-Kincaid grade level
    readability_score = textstat.flesch_kincaid_grade(text)

    # POS distribution
    pos_dist = Counter(token.pos_ for token in doc)

    # Named Entity Recognition using spaCy
    ner_entities = [ent.text for ent in doc.ents]
    ner_freq_dist = Counter(ner_entities)

    # Merge the dictionaries
    features = {
        'type_token_ratio': ttr,
        'sentence_complexity': sentence_complexity,
        'readability_score': readability_score,
        'pos_distribution': dict(pos_dist),
        'named_entity_frequency': dict(ner_freq_dist)
    }
    return features

def request_image(url, transform):
    rsp = requests.get(url, stream=True)
    img = Image.open(BytesIO(rsp.content)).convert('RGB')
    return transform(img)

def load_csv_dataset(file_path, image_size=(224, 224)):
    df = pd.read_csv(file_path, nrows=2400)
    transform = transforms.Compose([transforms.Resize(image_size), transforms.ToTensor()])
    processed_data = []

    for _, row in tqdm(df.iterrows(), total=df.shape[0]):
        story1 = row['sent1']
        story2 = row['sent2']
        label = row['label']

        # Parse the URL field which is in string representation of list
        urls = json.loads(row['url'].replace("'", '"'))

        img_tensors = []
        for url in urls:
            try:
                img_tensors.append(request_image(url, transform))
            except Exception as e:
                # print(f"Error downloading image: {e}")
                continue

        if len(img_tensors) == 0:
            continue

        img_tensor = torch.stack(img_tensors)

        processed_data.append({'image': img_tensor, 'story1': story1, 'story2': story2, 'label': label})

    return processed_data

def collate_fn(batch):
    # Pad the visual embeddings to have the same sequence length
    image_features = pad_sequence([item['visual_embeds'] for item in batch], batch_first=True)

    # Get the maximum sequence length for visual_attention_mask
    max_seq_length = max(item['visual_attention_mask'].shape[1] for item in batch)

    # Pad the visual_attention_mask tensors
    visual_attention_mask = [torch.cat([item['visual_attention_mask'], torch.zeros(item['visual_attention_mask'].shape[0], max_seq_length - item['visual_attention_mask'].shape[1])], dim=1) for item in batch]

    # Similarly, you can pad other sequences if necessary.
    # Stack other values
    input_ids_1 = torch.stack([item['input_ids_1'] for item in batch])
    attention_mask_1 = torch.stack([item['attention_mask_1'] for item in batch])
    visual_attention_mask = torch.stack(visual_attention_mask)
    ranker_gap = torch.tensor([item['ranker_gap'] for item in batch])
    # features1 = torch.stack([item['features1'] for item in batch])
    # features2 = torch.stack([item['features2'] for item in batch])

    ttr1 = torch.tensor([item['features1']['type_token_ratio'] for item in batch])
    ttr2 = torch.tensor([item['features2']['type_token_ratio'] for item in batch])

    sent_c_1 = torch.tensor([item['features1']['sentence_complexity'] for item in batch])
    sent_c_2 = torch.tensor([item['features2']['sentence_complexity'] for item in batch])

    read_1 = torch.tensor([item['features1']['readability_score'] for item in batch])
    read_2 = torch.tensor([item['features2']['readability_score'] for item in batch])

    coh_1 = torch.tensor([item['features1']['coherence_score'] for item in batch])
    coh_2 = torch.tensor([item['features2']['coherence_score'] for item in batch])

    ord_1 = torch.tensor([item['features1']['order_score'] for item in batch])
    ord_2 = torch.tensor([item['features2']['order_score'] for item in batch])

    coh_class_label = torch.tensor([item['coh_class_label'] for item in batch])

    return {
        'input_ids_1': input_ids_1,
        'attention_mask_1': attention_mask_1,
        'visual_embeds': image_features,
        'visual_attention_mask': visual_attention_mask,
        'ranker_gap': ranker_gap,
        'ttr1': ttr1,
        'ttr2': ttr2,
        'sent_c_1': sent_c_1,
        'sent_c_2': sent_c_2,
        'read_1': read_1,
        'read_2': read_2,
        'coh_1': coh_1,
        'coh_2': coh_2,
        'ord_1': ord_1,
        'ord_2': ord_2,
        'coh_class_label': coh_class_label
    }


# Custom Dataset class
class RankingDataset(Dataset):
    def __init__(self, data, tokenizer, resnet_model, max_length=512, mask_probability=0.15):
        self.data = data
        self.tokenizer = tokenizer
        self.resnet_model = resnet_model
        self.max_length = max_length
        self.mask_probability = mask_probability

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]

        # Process image
        image_feature_list = []
        # Extract features using ResNet
        for image in item['image']:
            with torch.no_grad():
                # Inside the __getitem__ method
                image_features = self.resnet_model(image.unsqueeze(0))
                # print("Shape after ResNet:", image_features.shape)

                image_features = image_features.view(image_features.size(0), -1)
                # print("Shape after flattening:", image_features.shape)

                image_feature_list.append(image_features)
                #image_features = self.projection(image_features)
                #print("Shape after projection:", image_features.shape)

        image_features = torch.cat(image_feature_list, dim=0)

        # Process text
        story1 = item['story1']
        story2 = item['story2']

        # Extract features
        features1 = extract_nlp_features(story1)
        features2 = extract_nlp_features(story2)


        # Extract COH scores
        # coh_scores1 = coherence_score(story1, coh_model, DEVICE)
        # coh_scores2 = coherence_score(story2, coh_model, DEVICE)

        features1['coherence_score'] = item['story1_coherence_score']
        features2['coherence_score'] = item['story2_coherence_score']

        # Extract order scores
        ord_scores1 = order_score(story1, 4)
        ord_scores2 = order_score(story2, 4)

        features1['order_score'] = ord_scores1
        features2['order_score'] = ord_scores2

        # Create combined text with features
        # combined_story1 = self.create_feature_text(story1, features1)
        # combined_story2 = self.create_feature_text(story2, features2)

        combined_input = story1 + ' [SEP] ' + story2

        # Tokenize text and prepare inputs for VisualBERT
        inputs_1 = self.tokenizer.encode_plus(
            combined_input,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        inputs_ids_1 = inputs_1['input_ids'].squeeze()

        attention_mask_1 = inputs_1['attention_mask']

        visual_attention_mask = torch.ones((image_features.size(0),), dtype=torch.long).unsqueeze(0)

        ranker_gap = item['label']

        coh_class_label = 1 if features1['coherence_score'] > features2['coherence_score'] else 0

        return {
            'input_ids_1': inputs_ids_1,
            'attention_mask_1': attention_mask_1,
            'visual_embeds': image_features,
            'visual_attention_mask': visual_attention_mask,
            'ranker_gap': ranker_gap,
            'features1': features1,
            'features2': features2,
            'coh_class_label': coh_class_label
        }

    def create_feature_text(self, story_text, features):
        # Format and append the features to the story text
        feature_text = f"{story_text} | TTR: {features['type_token_ratio']} | Complexity: {features['sentence_complexity']} | Readability: {features['readability_score']} | POS: {json.dumps(features['pos_distribution'])} | COH: {features['coherence_score']} | ORD: {features['order_score']}"
        return feature_text

# Initialize components
transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
resnet_model = models.resnet50(pretrained=True)
resnet_model = nn.Sequential(*list(resnet_model.children())[:-1])
resnet_model.eval()

print("Components initialized.")

# Load the dataset
print("Loading dataset...")

start = time.time()

loaded_data = []

len_load_data = 0

with h5py.File('/content/drive/MyDrive/11785_IDL/Project/VIST-Inspector-main/data/VHED/VHED_url.hdf5', 'r') as f:
    for group_key in f:
        group = f[group_key]
        item = {}
        for key in group:
            # Read the pickled data
            pickled_data = group[key][()].tobytes()
            # Deserialize using pickle
            item[key] = pickle.loads(pickled_data)
        loaded_data.append(item)
        # len_load_data += 1
        # if len_load_data >= 20:
        #     break

end = time.time()

print(f'Loaded {len(loaded_data)} items in {end - start} seconds')

# ranking_data = load_csv_dataset('/content/drive/MyDrive/Colab Notebooks/project/VHED_url.csv')

# ind = int(len(ranking_data) * 0.2)
# sub_loaded_data = ranking_data[:ind]
sub_loaded_data = loaded_data

random.seed(seed)

random.shuffle(sub_loaded_data)

train_data_ind = int(0.6 * len(sub_loaded_data))
val_data_ind = int(0.8 * len(sub_loaded_data))

train_data = sub_loaded_data[:train_data_ind]
val_data = sub_loaded_data[train_data_ind:val_data_ind]
test_data = sub_loaded_data[val_data_ind:]

# train_data = sub_loaded_data[:2]
# val_data = sub_loaded_data[3:5]
# test_data = sub_loaded_data[6:8]

train_data = calc_coh_for_data(train_data, coh_model, DEVICE)
val_data = calc_coh_for_data(val_data, coh_model, DEVICE)
test_data = calc_coh_for_data(test_data, coh_model, DEVICE)

train_dataset = RankingDataset(train_data, tokenizer, resnet_model)
val_dataset = RankingDataset(val_data, tokenizer, resnet_model)
test_dataset = RankingDataset(test_data, tokenizer, resnet_model)

train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn, num_workers=8)
val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn, num_workers=8)
test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn, num_workers=8)

# Training loop
def train_ranker(model, dataloader, optimizer, criterion, classification_criterion):
    model.train()

    total_correct = 0
    total_sample = 0

    for batch in tqdm(dataloader):
        input_ids_1 = batch['input_ids_1']
        attention_mask_1 = batch['attention_mask_1']
        visual_embeds = batch['visual_embeds']
        visual_attention_mask = batch['visual_attention_mask']
        ranking_gap = batch['ranker_gap'].float()
        # feature_text1 = batch['features1']
        # feature_text2 = batch['features2']

        ttr1 = batch['ttr1']
        ttr2 = batch['ttr2']

        sent_c_1 = batch['sent_c_1']
        sent_c_2 = batch['sent_c_2']

        read_1 = batch['read_1']
        read_2 = batch['read_2']

        coh_1 = batch['coh_1']
        coh_2 = batch['coh_2']

        ord_1 = batch['ord_1']
        ord_2 = batch['ord_2']

        coh_class_label = batch['coh_class_label']

        # move everything to device
        input_ids_1 = input_ids_1.to(DEVICE)
        attention_mask_1 = attention_mask_1.to(DEVICE)
        visual_embeds = visual_embeds.to(DEVICE)
        visual_attention_mask = visual_attention_mask.to(DEVICE)
        ranking_gap = ranking_gap.to(DEVICE)

        ttr1 = ttr1.to(DEVICE)
        ttr2 = ttr2.to(DEVICE)
        sent_c_1 = sent_c_1.to(DEVICE)
        sent_c_2 = sent_c_2.to(DEVICE)
        read_1 = read_1.to(DEVICE)
        read_2 = read_2.to(DEVICE)
        coh_1 = coh_1.to(DEVICE)
        coh_2 = coh_2.to(DEVICE)
        ord_1 = ord_1.to(DEVICE)
        ord_2 = ord_2.to(DEVICE)

        coh_class_label = coh_class_label.to(DEVICE)

        # Forward pass and loss computation for both story inputs
        ranking_prediction, classfy_pred = model(
            input_ids=input_ids_1,
            visual_embeds=visual_embeds,
            attention_mask=attention_mask_1,  # Add this
            visual_attention_mask=visual_attention_mask,  # Add this
            output_hidden_states=True
        )

        # compute accuracy
        for i in range(len(ranking_prediction)):
            if ranking_prediction[i] > 0 and ranking_gap[i] > 0:
                total_correct += 1
            elif ranking_prediction[i] < 0 and ranking_gap[i] < 0:
                total_correct += 1

        total_sample += ranking_prediction.shape[0]

        ranking_prediction = ranking_prediction.to(DEVICE)

        # Compute loss
        rank_loss = criterion(ranking_prediction, ranking_gap)

        classfy_loss = classification_criterion(classfy_pred, coh_class_label)

        total_loss = rank_loss * 0.5 + classfy_loss * 0.5
        total_loss = total_loss.float()

        optimizer.zero_grad()
        total_loss.backward()
        optimizer.step()

    print(f"Train Accuracy: {total_correct / total_sample}")
    print(f"Train Loss: {total_loss.item()}")

    torch.cuda.empty_cache()
    gc.collect()

def evaluate_ranker(model, dataloader, criterion):
    model.eval()
    total_correct = 0
    total_sample = 0
    total_loss = 0
    with torch.no_grad():
        for batch in tqdm(dataloader):
            input_ids_1 = batch['input_ids_1']
            attention_mask_1 = batch['attention_mask_1']
            visual_embeds = batch['visual_embeds']
            visual_attention_mask = batch['visual_attention_mask']
            ranking_gap = batch['ranker_gap'].float()
            # feature_text1 = batch['features1']
            # feature_text2 = batch['features2']

            ttr1 = batch['ttr1']
            ttr2 = batch['ttr2']

            sent_c_1 = batch['sent_c_1']
            sent_c_2 = batch['sent_c_2']

            read_1 = batch['read_1']
            read_2 = batch['read_2']

            coh_1 = batch['coh_1']
            coh_2 = batch['coh_2']

            ord_1 = batch['ord_1']
            ord_2 = batch['ord_2']

            #
            input_ids_1 = input_ids_1.to(DEVICE)
            attention_mask_1 = attention_mask_1.to(DEVICE)
            visual_embeds = visual_embeds.to(DEVICE)
            visual_attention_mask = visual_attention_mask.to(DEVICE)
            ranking_gap = ranking_gap.to(DEVICE)
            # feature_text1 = feature_text1.to(DEVICE)
            # feature_text2 = feature_text2.to(DEVICE)

            ttr1 = ttr1.to(DEVICE)
            ttr2 = ttr2.to(DEVICE)
            sent_c_1 = sent_c_1.to(DEVICE)
            sent_c_2 = sent_c_2.to(DEVICE)
            read_1 = read_1.to(DEVICE)
            read_2 = read_2.to(DEVICE)
            coh_1 = coh_1.to(DEVICE)
            coh_2 = coh_2.to(DEVICE)
            ord_1 = ord_1.to(DEVICE)
            ord_2 = ord_2.to(DEVICE)

            # Forward pass
            # outputs1 = model(
            #     input_ids=input_ids_1,
            #     visual_embeds=visual_embeds,
            #     attention_mask=attention_mask_1,  # Add this
            #     visual_attention_mask=visual_attention_mask,
            #     output_hidden_states=True
            # )

            # outputs_1 = model.dropout(outputs1.hidden_states[-1][:, 0])

            # # comb_out = combine_outputs(outputs_1, feature_text1, feature_text2)
            # # comb_out = torch.cat((outputs_1, ttr1, ttr2, sent_c_1, sent_c_2, coh_1, coh_2, ord_1[0], ord_1[1], ord_2[0], ord_2[1]),dim=-1)
            # # print(ttr1.shape)
            # # print(ttr2.shape)
            # # print(sent_c_1.shape)
            # # print(sent_c_2.shape)
            # # print(coh_1.shape)
            # # print(coh_2.shape)

            # ttr1 = ttr1.unsqueeze(1)
            # ttr2 = ttr2.unsqueeze(1)
            # sent_c_1 = sent_c_1.unsqueeze(1)
            # sent_c_2 = sent_c_2.unsqueeze(1)
            # coh_1 = coh_1.unsqueeze(1)
            # coh_2 = coh_2.unsqueeze(1)
            # # comb_out = torch.cat((outputs_1, ttr1, ttr2, sent_c_1, sent_c_2, coh_1, coh_2),dim=-1)
            # comb_out = torch.cat((outputs_1, coh_1, coh_2),dim=-1)

            # #
            # ranking_prediction_1 = model.regression_layer(comb_out.float())
            # ranking_prediction = ranking_prediction_1.squeeze(-1)

            # ranking_prediction = ranking_prediction.to(DEVICE)

            ranking_prediction, _ = model(
                input_ids=input_ids_1,
                visual_embeds=visual_embeds,
                attention_mask=attention_mask_1,  # Add this
                visual_attention_mask=visual_attention_mask,  # Add this
                output_hidden_states=True
            )
            #
            loss = criterion(ranking_prediction, ranking_gap)
            total_loss += loss.item()
            #
            for i in range(len(ranking_prediction)):
                if ranking_prediction[i] > 0 and ranking_gap[i] > 0:
                    total_correct += 1

                elif ranking_prediction[i] < 0 and ranking_gap[i] < 0:
                    total_correct += 1
            total_sample += ranking_prediction.shape[0]
    print(f"Val Accuracy: {total_correct / total_sample}")
    print(f"Val Loss: {total_loss / total_sample}")

    return total_correct / total_sample

# Load your pre-trained VisualBert model
model = VisualBertForPreTraining.from_pretrained('uclanlp/visualbert-vqa-coco-pre')
model.load_state_dict(torch.load('/content/drive/MyDrive/11785_IDL/Project/VIST-Inspector-main/model/epoch_1.pt'))
model.eval()  # Make sure the model is in evaluation mode

# Modify model for ranking task
# Add a linear layer to model for regression task
# model.regression_layer = nn.Linear(model.config.hidden_size + 2, 1)

model.dropout = nn.Dropout(p=0.1)

real_model = MultitaskModel(model, 2)

# Define optimizer and loss function for ranking
optimizer = torch.optim.Adam(model.parameters(), lr=1e-6, weight_decay=1e-5)
criterion = nn.MSELoss()
classification_criterion = nn.CrossEntropyLoss()

real_model = real_model.to(DEVICE)

num_epochs = 10

save_path = '/content/drive/MyDrive/11785_IDL/Project/VIST-Inspector-main/Rovist_pre_train_model/ranker_models'

best_val_acc = 0

for epoch in range(3, num_epochs):
    train_ranker(real_model, train_dataloader, optimizer, criterion, classification_criterion)
    val_acc = evaluate_ranker(real_model, val_dataloader, criterion)
    print(f"Epoch {epoch} complete.")
    # Save the fine-tuned model
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        torch.save(model.state_dict(), os.path.join(save_path, f'ranker_pair_best_epoch_{epoch}.pt'))

def test_ranker(model, dataloader, criterion):
    model.eval()
    total_correct = 0
    total_sample = 0
    total_loss = 0
    results = []
    with torch.no_grad():
        for batch in tqdm(dataloader):
            input_ids_1 = batch['input_ids_1']
            attention_mask_1 = batch['attention_mask_1']
            visual_embeds = batch['visual_embeds']
            visual_attention_mask = batch['visual_attention_mask']
            ranking_gap = batch['ranker_gap'].float()
            # feature_text1 = batch['features1']
            # feature_text2 = batch['features2']

            ttr1 = batch['ttr1']
            ttr2 = batch['ttr2']

            sent_c_1 = batch['sent_c_1']
            sent_c_2 = batch['sent_c_2']

            read_1 = batch['read_1']
            read_2 = batch['read_2']

            coh_1 = batch['coh_1']
            coh_2 = batch['coh_2']

            ord_1 = batch['ord_1']
            ord_2 = batch['ord_2']

            #
            input_ids_1 = input_ids_1.to(DEVICE)
            attention_mask_1 = attention_mask_1.to(DEVICE)
            visual_embeds = visual_embeds.to(DEVICE)
            visual_attention_mask = visual_attention_mask.to(DEVICE)
            ranking_gap = ranking_gap.to(DEVICE)
            # feature_text1 = feature_text1.to(DEVICE)
            # feature_text2 = feature_text2.to(DEVICE)

            ttr1 = ttr1.to(DEVICE)
            ttr2 = ttr2.to(DEVICE)
            sent_c_1 = sent_c_1.to(DEVICE)
            sent_c_2 = sent_c_2.to(DEVICE)
            read_1 = read_1.to(DEVICE)
            read_2 = read_2.to(DEVICE)
            coh_1 = coh_1.to(DEVICE)
            coh_2 = coh_2.to(DEVICE)
            ord_1 = ord_1.to(DEVICE)
            ord_2 = ord_2.to(DEVICE)

            # Forward pass
            outputs1 = model(
                input_ids=input_ids_1,
                visual_embeds=visual_embeds,
                attention_mask=attention_mask_1,  # Add this
                visual_attention_mask=visual_attention_mask,
                output_hidden_states=True
            )

            outputs_1 = model.dropout(outputs_1.hidden_states[-1][:, 0])

            ttr1 = ttr1.unsqueeze(1)
            ttr2 = ttr2.unsqueeze(1)
            sent_c_1 = sent_c_1.unsqueeze(1)
            sent_c_2 = sent_c_2.unsqueeze(1)
            coh_1 = coh_1.unsqueeze(1)
            coh_2 = coh_2.unsqueeze(1)

            # comb_out = combine_outputs(outputs_1, feature_text1, feature_text2)
            # comb_out = torch.cat((outputs_1, ttr1, ttr2, sent_c_1, sent_c_2, coh_1, coh_2, ord_1[0], ord_1[1], ord_2[0], ord_2[1]),dim=-1)
            # comb_out = torch.cat((outputs_1, ttr1, ttr2, sent_c_1, sent_c_2, coh_1, coh_2),dim=-1)
            comb_out = torch.cat((outputs_1, coh_1, coh_2),dim=-1)

            #
            ranking_prediction_1 = model.regression_layer(comb_out)
            ranking_prediction = ranking_prediction_1.squeeze(-1)

            results.extend(ranking_prediction.tolist())

            ranking_prediction = ranking_prediction.to(DEVICE)
            #
            loss = criterion(ranking_prediction, ranking_gap)
            total_loss += loss.item()
            #
            for i in range(len(ranking_prediction)):
                if ranking_prediction[i] > 0 and ranking_gap[i] > 0:
                    total_correct += 1

                elif ranking_prediction[i] < 0 and ranking_gap[i] < 0:
                    total_correct += 1
            total_sample += ranking_prediction.shape[0]
    print(f"Test Accuracy: {total_correct / total_sample}")
    print(f"Test Loss: {total_loss / total_sample}")

    return results

finetuned_model = VisualBertForPreTraining.from_pretrained('uclanlp/visualbert-vqa-coco-pre')

finetuned_model.regression_layer = nn.Linear(finetuned_model.config.hidden_size + 2, 1)

finetuned_model.load_state_dict(torch.load('/content/drive/MyDrive/11785_IDL/Project/VIST-Inspector-main/Rovist_pre_train_model/ranker_models/ranker_pair_best_epoch_3.pt'))

finetuned_model = finetuned_model.to(DEVICE)

results = test_ranker(finetuned_model, test_dataloader, criterion)

import csv

# write test dataloader column story1, story2, label and the above reults to csv
with open('/content/drive/MyDrive/11785_IDL/Project/VIST-Inspector-main/Rovist_pre_train_model/ranker_w_ling_pair_test_results.csv', 'w') as f:
    writer = csv.writer(f)

    writer.writerow(['story1', 'story2', 'label', 'prediction'])

    data = test_dataloader.dataset.data

    for i in range(len(data)):
        writer.writerow([data[i]['story1'], data[i]['story2'], data[i]['label'], results[i]])

    f.close()
    print('Done!')
